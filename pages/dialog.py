import streamlit as st
import uuid
from streamlit_chatbox import ChatBox
from configs.model import TEMPERATURE, HISTORY_LEN
from configs.prompt import PROMPT_TEMPLATES
from pages.api_utils import ApiRequest
from pages.components import render_conversation_management, render_model_config, render_mode_specific_config

chat_box = ChatBox()


def dialogue_page(api: ApiRequest, is_lite: bool = False):
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    st.session_state.setdefault("conversation_ids", {})
    st.session_state["conversation_ids"].setdefault(chat_box.cur_chat_name, uuid.uuid4().hex)
    st.session_state.setdefault("file_chat_id", None)

    # æ£€æŸ¥è¿è¡Œä¸­æ¨¡å‹
    running_models = api.list_running_models()
    if not running_models:
        st.toast("âš ï¸ å½“å‰æ²¡æœ‰è¿è¡Œä¸­çš„æ¨¡å‹ï¼Œè¯·å…ˆå¯åŠ¨æ¨¡å‹ã€‚")
        return
    default_model = list(running_models.keys())[0]

    # åˆå§‹åŒ–æç¤º
    if not chat_box.chat_inited:
        st.toast(f"ğŸ‘‹ æ¬¢è¿ä½¿ç”¨ChatBot\nå½“å‰è¿è¡Œæ¨¡å‹: {default_model}")
        chat_box.init_session()

    # é¡µé¢å¸ƒå±€ï¼šå·¦ä¾§ä¾§è¾¹æ ï¼ˆé…ç½®ï¼‰ï¼Œå³ä¾§ä¸»å†…å®¹ï¼ˆèŠå¤©ï¼‰
    col_sidebar, col_main = st.columns(
        [1, 4],  # ä¾§è¾¹æ :ä¸»å†…å®¹ = 1:4ï¼Œæ‰©å¤§ä¸»å†…å®¹åŒº
        gap="large"  # å¢åŠ åˆ—ä¹‹é—´çš„é—´éš™
    )

    with col_sidebar:
        # å¯¹è¯æ¨¡å¼é€‰æ‹©ï¼ˆæ ¸å¿ƒå·®å¼‚åŒ–å…¥å£ï¼‰
        dialogue_modes = ["LLM å¯¹è¯", "çŸ¥è¯†åº“é—®ç­”", "æ–‡ä»¶å¯¹è¯", "æœç´¢å¼•æ“é—®ç­”", "è‡ªå®šä¹‰Agenté—®ç­”"]
        dialogue_mode = st.selectbox(
            "å¯¹è¯æ¨¡å¼",
            dialogue_modes,
            index=0,
            key="dialogue_mode",
            format_func=lambda x: f"ğŸ“Œ {x}"  # åŠ å›¾æ ‡åŒºåˆ†
        )

        # æ¸²æŸ“å…¬å…±ç»„ä»¶
        conversation_id = render_conversation_management(chat_box, st.session_state["conversation_ids"])
        llm_model, prompt_template, temperature, history_len = render_model_config(
            running_models=list(running_models.keys()),
            dialogue_mode=dialogue_mode,
            prompt_templates=PROMPT_TEMPLATES
        )

        # æ¸²æŸ“æ¨¡å¼ä¸“å±é…ç½®
        mode_config = render_mode_specific_config(dialogue_mode, api)

    with col_main:
        # ä¸»æ ‡é¢˜ï¼ˆå¸¦æ¨¡å¼æ ‡è¯†ï¼‰
        mode_icons = {
            "LLM å¯¹è¯": "ğŸ’¬",
            "çŸ¥è¯†åº“é—®ç­”": "ğŸ“š",
            "æ–‡ä»¶å¯¹è¯": "ğŸ“‚",
            "æœç´¢å¼•æ“é—®ç­”": "ğŸ”",
            "è‡ªå®šä¹‰Agenté—®ç­”": "ğŸ¤–"
        }
        st.header(f"{mode_icons[dialogue_mode]} {dialogue_mode}", divider="blue")

        # èŠå¤©åŒºåŸŸ
        chat_box.output_messages()

        # è¾“å…¥åŒºåŸŸï¼ˆæ ¹æ®æ¨¡å¼è°ƒæ•´æç¤ºï¼‰
        placeholders = {
            "LLM å¯¹è¯": "è¯·è¾“å…¥å¯¹è¯å†…å®¹...",
            "çŸ¥è¯†åº“é—®ç­”": "è¯·è¾“å…¥å…³äºçŸ¥è¯†åº“çš„é—®é¢˜...",
            "æ–‡ä»¶å¯¹è¯": "è¯·è¾“å…¥å…³äºä¸Šä¼ æ–‡ä»¶çš„é—®é¢˜...",
            "æœç´¢å¼•æ“é—®ç­”": "è¯·è¾“å…¥éœ€è¦æœç´¢çš„é—®é¢˜...",
            "è‡ªå®šä¹‰Agenté—®ç­”": "è¯·è¾“å…¥éœ€è¦Agentå¤„ç†çš„é—®é¢˜..."
        }
        prompt = st.chat_input(placeholders[dialogue_mode], key="prompt")

        if prompt:
            # æ˜¾ç¤ºåŠ è½½çŠ¶æ€
            with st.spinner("æ€è€ƒä¸­..."):
                # æ ¹æ®æ¨¡å¼è°ƒç”¨ä¸åŒAPI
                if dialogue_mode == "LLM å¯¹è¯":
                    chat_box.ai_say("æ­£åœ¨ç”Ÿæˆå›ç­”...")
                    text = ""
                    message_id = ""
                    # æµå¼è·å–ç»“æœ
                    r = api.chat_chat(
                        prompt,
                        history=[],
                        conversation_id=conversation_id,
                        model=llm_model,
                        prompt_name=prompt_template,
                        temperature=temperature
                    )
                    for t in r:
                        text += t.get("text", "")
                        chat_box.update_msg(text)
                        message_id = t.get("message_id", "")
                    # æ›´æ–°æœ€ç»ˆç»“æœ
                    chat_box.update_msg(text, streaming=False, metadata={"message_id": message_id})